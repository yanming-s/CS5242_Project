import pandas as pd
from pathlib import Path
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import matplotlib.image as mpimg
from collections import defaultdict
import random
from collections import Counter
import seaborn as sns
from torch.utils.data import Dataset
import torchvision.transforms as T
from glob import glob
from torch.utils.data import DataLoader, WeightedRandomSampler
import os

def balanced_multilabel_split(df, train_size=20000, test_size=10000, seed=42):
    random.seed(seed)

    # Convert string labels to list
    if isinstance(df['Finding Labels'].iloc[0], str):
        df['Finding Labels'] = df['Finding Labels'].str.split('|')

    # Count label frequencies
    label_counter = Counter(label for labels in df['Finding Labels'] for label in labels)
    sorted_labels = sorted(label_counter.items(), key=lambda x: x[1])  # rarest first

    # Build mapping
    image_labels = dict(zip(df['Image Index'], df['Finding Labels']))
    label_to_images = defaultdict(set)
    for img, labels in image_labels.items():
        for label in labels:
            label_to_images[label].add(img)

    # Sets to track assignment
    train_images = set()
    test_images = set()
    used_images = set()

    def add_image(img, to_train):
        target_set = train_images if to_train else test_images
        if img not in used_images:
            target_set.add(img)
            used_images.add(img)

    for label, _ in sorted_labels:
        candidate_images = list(label_to_images[label] - used_images)
        random.shuffle(candidate_images)

        n_total = len(candidate_images)

        # Skip if we've hit capacity
        if len(train_images) >= train_size and len(test_images) >= test_size:
            break

        remaining_train = train_size - len(train_images)
        remaining_test = test_size - len(test_images)
        available = min(n_total, remaining_train + remaining_test)

        if available == 0:
            continue

        n_train = int(available * 0.8)
        n_test = available - n_train

        # Adjust to stay within global limits
        n_train = min(n_train, remaining_train)
        n_test = min(n_test, remaining_test)

        # Assign images
        for img in candidate_images[:n_train]:
            add_image(img, to_train=True)
        for img in candidate_images[n_train:n_train + n_test]:
            add_image(img, to_train=False)

    return list(train_images), list(test_images)

def binary_balanced_split(df, train_ratio=0.8, seed=42):
    """
    Manually split a balanced dataset into train and test.
    Returns train_df and test_df.
    """
    random.seed(seed)

    # Ensure 'binary_label' column exists
    if 'binary_label' not in df.columns:
        df['binary_label'] = df['Finding Labels'].apply(lambda labels: 0 if labels == ['No Finding'] else 1)

    # Separate into normal and abnormal
    df_normal = df[df['binary_label'] == 0].copy()
    df_abnormal = df[df['binary_label'] == 1].copy()

    # Match dataset sizes by downsampling the larger group
    min_size = min(len(df_normal), len(df_abnormal))
    df_normal = df_normal.sample(min_size, random_state=seed)
    df_abnormal = df_abnormal.sample(min_size, random_state=seed)

    # Shuffle both
    df_normal = df_normal.sample(frac=1, random_state=seed).reset_index(drop=True)
    df_abnormal = df_abnormal.sample(frac=1, random_state=seed).reset_index(drop=True)

    # Calculate split indices
    n_train = int(train_ratio * min_size)

    # Split each group
    train_normal = df_normal.iloc[:n_train]
    test_normal = df_normal.iloc[n_train:]

    train_abnormal = df_abnormal.iloc[:n_train]
    test_abnormal = df_abnormal.iloc[n_train:]

    # Combine splits
    train_df = pd.concat([train_normal, train_abnormal]).sample(frac=1, random_state=seed).reset_index(drop=True)
    test_df = pd.concat([test_normal, test_abnormal]).sample(frac=1, random_state=seed).reset_index(drop=True)

    return train_df, test_df

def get_label_distribution(subset_df):
    all_labels = [label for labels in subset_df['Finding Labels'] for label in labels]
    return Counter(all_labels)

def get_all_image_paths(root_dir):
    """
    Scans all `images_*/images/*.png` subfolders and returns a dictionary
    mapping image filename to full path. Fits to the structure of the dataset when downloaded
    """
    image_dict = {}
    # Match all folders like images_001, images_002, ..., images_012
    folders = glob(os.path.join(root_dir, 'images_*', 'images'))

    for folder in folders:
        all_images = glob(os.path.join(folder, '*.png'))
        for path in all_images:
            name = os.path.basename(path)
            image_dict[name] = path

    return image_dict

def plot_class_balance(train_df, test_df):
    # Get label distributions
    train_counts = get_label_distribution(train_df)
    test_counts = get_label_distribution(test_df)

    # Combine into a DataFrame for seaborn
    all_labels = sorted(set(train_counts.keys()).union(test_counts.keys()))
    data = {
        'Class': all_labels,
        'Train': [train_counts.get(label, 0) for label in all_labels],
        'Test': [test_counts.get(label, 0) for label in all_labels],
    }
    balance_df = pd.DataFrame(data)

    melt_df = pd.melt(balance_df, id_vars='Class', value_vars=['Train', 'Test'],
                      var_name='Dataset', value_name='Image Count')

    # Plot
    plt.figure(figsize=(12, 6))
    sns.barplot(data=melt_df, x='Class', y='Image Count', hue='Dataset')
    plt.title('Class Balance in Train vs Test Sets')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

class NIHDataset_modified(Dataset):
    def __init__(self, df, root_dir, sizeimagesout, train=True):
        self.sizeimagesout = sizeimagesout
        self.root_dir = root_dir
        df['Finding Labels'] = df['Finding Labels'].apply(lambda x: x.split('|') if isinstance(x, str) else x)
        image_dict = get_all_image_paths(root_dir)

        self.image_names = [name for name in df['Image Index'] if name in image_dict]
        self.image_paths = [image_dict[name] for name in self.image_names]
        self.labels = df.set_index('Image Index').loc[self.image_names]['Finding Labels'].tolist()
        self.ages = df.set_index('Image Index').loc[self.image_names]['Patient Age'].tolist()
        if train:
            self.transform = T.Compose([
                T.RandomRotation(10, expand=True),
                T.RandomHorizontalFlip(),
                T.RandomVerticalFlip(),
                T.Resize(self.sizeimagesout),
                T.ToTensor(),
                T.Normalize(mean=[0.5], std=[0.5])
            ])
        else:
            self.transform = T.Compose([
                T.Resize(self.sizeimagesout),
                T.ToTensor(),
                T.Normalize(mean=[0.5], std=[0.5])
            ])


    def __len__(self):
        return len(self.image_names)
    
    def __repr__(self):
        return (f"NIHDataset_modified(num_samples={len(self)}, "
                f"image_size={self.sizeimagesout}, "
                f"path='{self.root_dir}')")

    def __getitem__(self, index):
        img_path = self.image_paths[index]
        img = Image.open(img_path).convert("L")
        img = self.transform(img)
        return img, self.labels[index], self.ages[index]
    
class NIHBinaryDataset(Dataset):
    def __init__(self, df, root_dir, sizeimagesout, train=True):
        self.df = df.reset_index(drop=True)
        self.root_dir = root_dir
        self.sizeimagesout = sizeimagesout

        # Make sure label is binary
        if 'binary_label' not in self.df.columns:
            self.df['binary_label'] = self.df['Finding Labels'].apply(lambda labels: 0 if labels == ['No Finding'] else 1)
        
        # Map images to full paths
        self.image_paths = get_all_image_paths(root_dir)  # This should return {filename: full_path}
        self.df = self.df[self.df['Image Index'].isin(self.image_paths.keys())]
        if train:
            self.transform = T.Compose([
                T.RandomRotation(10, expand=True),
                T.RandomHorizontalFlip(),
                T.RandomVerticalFlip(),
                T.Resize(self.sizeimagesout),
                T.ToTensor(),
                T.Normalize(mean=[0.5], std=[0.5])
            ])
        else:
            self.transform = T.Compose([
                T.Resize(self.sizeimagesout),
                T.ToTensor(),
                T.Normalize(mean=[0.5], std=[0.5])
            ])

        # Filter image paths accordingly
        self.df['full_path'] = self.df['Image Index'].map(self.image_paths)

    def __len__(self):
        return len(self.df)

    def __repr__(self):
        return (f"NIHBinaryDataset(num_samples={len(self)}, "
                f"image_size={self.sizeimagesout}, "
                f"path='{self.root_dir}')")

    def __getitem__(self, index):
        row = self.df.iloc[index]
        image = Image.open(row['full_path']).convert("L")
        image = T.Resize(self.sizeimagesout)(image)
        label = row['binary_label']
        age = row['Patient Age']

        return image, label, age

def save_dataset_stats(df, filename):
    stats = {
        'num_samples': len(df),
        'mean_age': df['Patient Age'].mean(),
        'min_age': df['Patient Age'].min(),
        'max_age': df['Patient Age'].max(),
        'label_distribution': df['binary_label'].value_counts().to_dict()
    }
    pd.Series(stats).to_json(filename, indent=4)


if __name__ == "__main__":
    root_dir = 'C:/Users/e1498134/Documents/Courses/CS5242/Project'
    df = pd.read_csv(root_dir+'/Data_Entry_2017.csv')
    df = df[df['Patient Age'] <= 100].reset_index(drop=True)
    df['Finding Labels'] = df['Finding Labels'].str.split('|')

    size_out = (256, 256)

    ## Dataset constitution:
    # training set = 20000 
    # test set ~ 8000 
    # includes the age in the data (img, label[], age)
    train_ids, test_ids = balanced_multilabel_split(df, train_size=20000, test_size=10000)

    train_df = df[df['Image Index'].isin(train_ids)].reset_index(drop=True)
    test_df = df[df['Image Index'].isin(test_ids)].reset_index(drop=True)

    train_dataset = NIHDataset_modified(train_df, root_dir, size_out)
    test_dataset = NIHDataset_modified(test_df, root_dir, size_out, train=False)

    '''print("Train label distribution:")
    print(get_label_distribution(train_df))
    print("\nTest label distribution:")
    plot_class_balance(train_df, test_df)'''
    print(get_label_distribution(test_df))
    img, label, age = train_dataset[0]
    imgplot = plt.imshow(np.squeeze(img))
    plt.show()

    ## Dataset constitution for Transformer: Label ∈ (Normal, Abnormal)
    # training set = 82800 
    # test set = 20702  
    # includes the age in the data (img, label, age)

    train_df_binary, test_df_binary = binary_balanced_split(df)
    train_dataset_binary = NIHBinaryDataset(train_df_binary, root_dir,size_out)
    test_dataset_binary = NIHBinaryDataset(test_df_binary, root_dir, size_out, train=False)

    print(f"Train set: {len(train_df_binary)} (Normal: {sum(train_df_binary['binary_label']==0)}, Abnormal: {sum(train_df_binary['binary_label']==1)})")
    print(f"Test set: {len(test_df_binary)} (Normal: {sum(test_df_binary['binary_label']==0)}, Abnormal: {sum(test_df_binary['binary_label']==1)})")
    img, label, age = train_dataset_binary[0]
    imgplot = plt.imshow(np.squeeze(img))
    plt.show()

    ### Save train/test splits to CSV for reproducibility
    # train_df_binary.to_csv("train_split_binary.csv", index=False)
    #test_df_binary.to_csv("test_split_binary.csv", index=False)
    #train_df.to_csv("train_split_multilabel.csv", index=False)
    #test_df.to_csv("test_split_multilabel.csv", index=False)

    ### Save dataset statistics:
    #save_dataset_stats(train_df_binary, "train_binary_stats.json")
    #save_dataset_stats(test_df_binary, "test_binary_stats.json")

    ### Add a weighted sampler
    # Compute weights
    #class_counts = train_df_binary['binary_label'].value_counts().to_dict()
    #weights = train_df_binary['binary_label'].map(lambda x: 1.0 / class_counts[x]).values
    #sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)
    # DataLoader
    #train_loader = DataLoader(train_dataset, sampler=sampler, batch_size=32)
