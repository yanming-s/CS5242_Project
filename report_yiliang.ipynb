{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98cf2de",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Models\n",
    "\n",
    "Convoluational neural networks (CNNs) have long been the foundation of computer vision. Although surpassed by transformer-based models in large-scale and data-rich settings, they remain competitibe for efficiency and resource-constrained applications. We implemented several CNN models, which include AlexNet, VGG-16, ResNet-18, and ResNet-34. We then compared the performance of different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e7c08",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ab1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model_args = {\n",
    "    'in_channels': 1,\n",
    "    'num_classes': 15\n",
    "}\n",
    "\n",
    "''' Get the number of parameters of a model '''\n",
    "def get_num_params(model: nn.Module):\n",
    "  return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30732024",
   "metadata": {},
   "source": [
    "#### 1. AlexNet\n",
    "\n",
    "AlexNet consists of five convolutional layers followed by three fully connected layers, using large receptive fields, ReLU activations, overlapping max-pooling, and dropout to improve training efficiency and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac49333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in AlexNet: 57049807\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(f\"Number of parameters in AlexNet: {get_num_params(AlexNet(**model_args))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3bd84",
   "metadata": {},
   "source": [
    "#### 2. VGG\n",
    "\n",
    "VGG adopts a simple and uniform architecture that stacks multiple small 3Ã—3 convolutional layers with ReLU interleaved with max-pooling layers, followed by three fully connected layers for classification, emphasizing depth and simplicity over large convolutional kernels. VGG has different variants, such as VGG-11, VGG-13, VGG-16, and VGG-19, which differ in the number of convolutional layers stacked within each block. This offers a trade-off between model depth and computational cost, while maintaing a uniform overall architecture. We leveraged this characteristic to efficiently implement different VGG variants with one unified class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbe0fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in VGG-11: 128826639\n",
      "Number of parameters in VGG-13: 129011151\n",
      "Number of parameters in VGG-16: 134320847\n",
      "Number of parameters in VGG-19: 139630543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    class Variant(Enum):\n",
    "        VGG11 = '11'\n",
    "        VGG13 = '13'\n",
    "        VGG16 = '16'\n",
    "        VGG19 = '19'\n",
    "\n",
    "    _architectures = {\n",
    "        Variant.VGG11: [\n",
    "            64, 'M',\n",
    "            128, 'M',\n",
    "            256, 256, 'M',\n",
    "            512, 512, 'M',\n",
    "            512, 512, 'M'\n",
    "        ],\n",
    "        Variant.VGG13: [\n",
    "            64, 64, 'M',\n",
    "            128, 128, 'M',\n",
    "            256, 256, 'M',\n",
    "            512, 512, 'M',\n",
    "            512, 512, 'M'\n",
    "        ],\n",
    "        Variant.VGG16: [\n",
    "            64, 64, 'M',\n",
    "            128, 128, 'M',\n",
    "            256, 256, 256, 'M',\n",
    "            512, 512, 512, 'M',\n",
    "            512, 512, 512, 'M'\n",
    "        ],\n",
    "        Variant.VGG19: [\n",
    "            64, 64, 'M',\n",
    "            128, 128, 'M',\n",
    "            256, 256, 256, 256, 'M',\n",
    "            512, 512, 512, 512, 'M',\n",
    "            512, 512, 512, 512, 'M'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, variant='16'):\n",
    "        super(VGG, self).__init__()\n",
    "        if not isinstance(variant, self.Variant):\n",
    "            variant = self.Variant(variant)\n",
    "        self.conv_layers = self._make_conv_layers(\n",
    "            in_channels,\n",
    "            self._architectures[variant]\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_conv_layers(self, in_channels, arch):\n",
    "        layers = []\n",
    "        channels = in_channels\n",
    "        for x in arch:\n",
    "            if type(x) == int:\n",
    "                layers.append(nn.Conv2d(channels, x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                channels = x\n",
    "            elif x == 'M':\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                raise ValueError(f\"VGG unknown layer: {x}\")\n",
    "        layers.append(nn.AdaptiveAvgPool2d((7, 7)))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "for variant in ['11', '13', '16', '19']:\n",
    "    print(f\"Number of parameters in VGG-{variant}: {get_num_params(VGG(**model_args, variant=variant))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140c741",
   "metadata": {},
   "source": [
    "#### 3. ResNet\n",
    "\n",
    "ResNet introduces residual connections to mitigate the vanishing gradient problem, enabling training very deep networks effectively. Similar to VGG, it is structured with stacks of residual blocks, with popular variants like ResNet-18, ResNet-35, ResNet-50, ResNet-101 ,and ResNet-152, differing both in depth and block structure. We implemented ResNet-18 and ResNet-34 as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de53e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in ResNet-18: 11177935\n",
      "Number of parameters in ResNet-34: 21286095\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    class Variant(Enum):\n",
    "        RESNET18 = '18'\n",
    "        RESNET34 = '34'\n",
    "\n",
    "    _architectures = {\n",
    "        Variant.RESNET18: (BasicBlock, [2, 2, 2, 2]),\n",
    "        Variant.RESNET34: (BasicBlock, [3, 4, 6, 3])\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, variant='18'):\n",
    "        super(ResNet, self).__init__()\n",
    "        if not isinstance(variant, self.Variant):\n",
    "            variant = self.Variant(variant)\n",
    "        block, layers = self._architectures[variant]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "for variant in ['18', '34']:\n",
    "    print(f\"Number of parameters in ResNet-{variant}: {get_num_params(ResNet(**model_args, variant=variant))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55496634",
   "metadata": {},
   "source": [
    "### Experimental Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4080db",
   "metadata": {},
   "source": [
    "We conducted experiments to evaluate and comapre the models using the metrics discussed earlier. Also, we applied the management schemes discussed in the previous section during training.\n",
    "\n",
    "Using different sampling strategy, we evaluated the performance of different models, as shown in the following tables.\n",
    "\n",
    "Original:\n",
    "\n",
    "|      Model        | Predict Acc (%) | Predict Recall (%) |\n",
    "| :---------------: | :-------------: | :----------------: |\n",
    "|     AlexNet       |     68.70%      |      57.38%        |\n",
    "|     VGG-16        |     67.31%      |      61.78%        |\n",
    "|     ResNet-18     |     66.47%      |      65.23%        |\n",
    "|     ResNet-34     |     68.02%      |      65.32%        |\n",
    "\n",
    "Round-robin:\n",
    "\n",
    "|      Model        | Predict Acc (%) | Predict Recall (%) |\n",
    "| :---------------: | :-------------: | :----------------: |\n",
    "|     AlexNet       |     92.40%      |      98.97%        |\n",
    "|     VGG-16        |     92.84%      |      100.00%       |\n",
    "|     ResNet-18     |     92.88%      |      100.00%       |\n",
    "|     ResNet-34     |     92.80%      |      99.91%        |\n",
    "\n",
    "Rare-first:\n",
    "\n",
    "|      Model        | Predict Acc (%) | Predict Recall (%) |\n",
    "| :---------------: | :-------------: | :----------------: |\n",
    "|     AlexNet       |     100.00%     |     100.00%        |\n",
    "|     VGG-16        |     100.00%     |     100.00%        |\n",
    "|     ResNet-18     |     100.00%     |     100.00%        |\n",
    "|     ResNet-34     |     100.00%     |     100.00%        |\n",
    "\n",
    "First of all, we observe that rare-first sampling and round-robin sampling consistently help all models improve their predict accuracy and recall rate, demonstrating the effectiveness of our preprocessing for the unbalanced data. In the context of medical diagnosis, this improvement in accuracy and recall rate can be critical for practical application.\n",
    "\n",
    "Training time per epoch on NVIDIA H100:\n",
    "\n",
    "|      Model            | Training Time (s/epoch) |\n",
    "| :---------------:     | :---------------------: |\n",
    "|     AlexNet           |     3.7                 |\n",
    "|     VGG-16            |     21.0                |\n",
    "|     ResNet-18         |     6.1                 |\n",
    "|     ResNet-34         |     9.0                 |\n",
    "\n",
    "When comparing different models, similar performance is observed. Especiallly, the number of parameters in VGG-16 is much greater than other models, resulting in significantly longer training time. However, it doesn't show the best performance. In contrast, AlexNet can provide comparable performence with a relatively short training time. This result indicates that more parameters do not necessarily result in a better performance. When designing a deep learning system, we need to choose the architecture or model based on the scale and quality of the dataset and specific application scenarios.\n",
    "\n",
    "Minimum training loss and validation loss at early stopping (round-robin sampling):\n",
    "\n",
    "|      Model        | Traning Loss    | Validation Loss  |\n",
    "| :---------------: | :-------------: | :--------------: |\n",
    "|     AlexNet       |     0.177       |      0.313       |\n",
    "|     VGG-16        |     0.038       |      0.729       |\n",
    "|     ResNet-18     |     0.042       |      0.396       |\n",
    "|     ResNet-34     |     0.038       |      0.431       |\n",
    "\n",
    "The above table shows the traning loss and validation loss of differnt models when the training early stopped. With a larger number of parameters, VGG-16 has better learning ability than other models, which results in a smallest training loss. However, with limited training data, this also leads to a more obvious trend of overfitting, yielding a highest observed validation loss. This again validates our previous argument that it may not always be beneficial to have more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f050a9",
   "metadata": {},
   "source": [
    "## Transformers Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd4799",
   "metadata": {},
   "source": [
    "### Pretrained ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126eee1",
   "metadata": {},
   "source": [
    "We encapsulated the ViT model from the `transformers` package to adapt it to our available pipeline. The model was fine-tuned using our preprocessed dataset after loading the pre-trained weights from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTConfig, ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "\n",
    "class ViT_PT(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, base_model_name=\"google/vit-base-patch16-224\"):\n",
    "        super().__init__()\n",
    "        config = ViTConfig.from_pretrained(base_model_name)\n",
    "        config.num_channels = in_channels\n",
    "        config.num_labels = num_classes\n",
    "        config.ignore_mismatched_sizes = True\n",
    "        self.config = config\n",
    "        self.tokenizer = ViTImageProcessor.from_pretrained(\n",
    "            base_model_name, do_resize=False, do_rescale=True, do_normalize=False)\n",
    "        self.base_model = ViTForImageClassification(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[2] == self.config.image_size and x.shape[3] == self.config.image_size\n",
    "        # x: [bs, in_channels, img_size, img_size]\n",
    "        # Resize\n",
    "        inputs = self.tokenizer.preprocess(images=x, data_format=\"channels_first\",\n",
    "                                           input_data_format=\"channels_first\", return_tensors='pt')\n",
    "        d = x.get_device()\n",
    "        if d != -1:\n",
    "            inputs = inputs.to(d)\n",
    "\n",
    "        outputs = self.base_model(**inputs)\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec874b",
   "metadata": {},
   "source": [
    "The results for the fine-tuned ViT are shown in the following table shows the results for the ViT model after fine-tuning:\n",
    "\n",
    "| Sampling Strategy | Predict Acc (%) | Predict Recall (%) |\n",
    "| :---------------: | :-------------: | :----------------: |\n",
    "|     Original      |     65.31%      |       57.20%       |\n",
    "|    Round-robin    |     95.88%      |      100.00%       |\n",
    "|    Rare-first     |     100.00%     |      100.00%       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5242project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
