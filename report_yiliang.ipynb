{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98cf2de",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Models\n",
    "\n",
    "Convoluational neural networks (CNNs) have long been the foundation of computer vision. Although surpassed by transformer-based models in large-scale and data-rich settings, they remain competitibe for efficiency and resource-constrained applications. We implemented several CNN models, which include AlexNet, VGG-16, ResNet-18, and ResNet-34. We then compared the performance of different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e7c08",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30732024",
   "metadata": {},
   "source": [
    "#### 1. AlexNet\n",
    "\n",
    "AlexNet consists of five convolutional layers followed by three fully connected layers, using large receptive fields, ReLU activations, overlapping max-pooling, and dropout to improve training efficiency and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac49333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3bd84",
   "metadata": {},
   "source": [
    "#### 2. VGG\n",
    "\n",
    "VGG adopts a simple and uniform architecture that stacks multiple small 3Ã—3 convolutional layers with ReLU interleaved with max-pooling layers, followed by three fully connected layers for classification, emphasizing depth and simplicity over large convolutional kernels. VGG has different variants, such as VGG11, VGG13, VGG16, and VGG19, which differ in the number of convolutional layers stacked within each block. This offers a trade-off between model depth and computational cost, while maintaing a uniform overall architecture. We leveraged this characteristic to efficiently implement different VGG variants with one unified class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    class Variant(Enum):\n",
    "        VGG11 = '11'\n",
    "        VGG13 = '13'\n",
    "        VGG16 = '16'\n",
    "        VGG19 = '19'\n",
    "\n",
    "    _architectures = {\n",
    "        Variant.VGG11: [\n",
    "            64, 'M',\n",
    "            128, 'M',\n",
    "            256, 256, 'M',\n",
    "            512, 512, 'M',\n",
    "            512, 512, 'M'\n",
    "        ],\n",
    "        Variant.VGG13: [\n",
    "            64, 64, 'M',\n",
    "            128, 128, 'M',\n",
    "            256, 256, 'M',\n",
    "            512, 512, 'M',\n",
    "            512, 512, 'M'\n",
    "        ],\n",
    "        Variant.VGG16: [\n",
    "            64, 64, 'M',\n",
    "            128, 128, 'M',\n",
    "            256, 256, 256, 'M',\n",
    "            512, 512, 512, 'M',\n",
    "            512, 512, 512, 'M'\n",
    "        ],\n",
    "        Variant.VGG19: [\n",
    "            64, 64, 'M',\n",
    "            128, 128, 'M',\n",
    "            256, 256, 256, 256, 'M',\n",
    "            512, 512, 512, 512, 'M',\n",
    "            512, 512, 512, 512, 'M'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, variant='16'):\n",
    "        super(VGG, self).__init__()\n",
    "        if not isinstance(variant, self.Variant):\n",
    "            variant = self.Variant(variant)\n",
    "        self.conv_layers = self._make_conv_layers(\n",
    "            in_channels,\n",
    "            self._architectures[variant]\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_conv_layers(self, in_channels, arch):\n",
    "        layers = []\n",
    "        channels = in_channels\n",
    "        for x in arch:\n",
    "            if type(x) == int:\n",
    "                layers.append(nn.Conv2d(channels, x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                channels = x\n",
    "            elif x == 'M':\n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                raise ValueError(f\"VGG unknown layer: {x}\")\n",
    "        layers.append(nn.AdaptiveAvgPool2d((7, 7)))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140c741",
   "metadata": {},
   "source": [
    "#### 3. ResNet\n",
    "\n",
    "ResNet introduces residual connections to mitigate the vanishing gradient problem, enabling training very deep networks effectively. Similar to VGG, it is structured with stacks of residual blocks, with popular variants like ResNet-18, ResNet-35, ResNet-50, ResNet-101 ,and ResNet-152, differing both in depth and block structure. We implemented ResNet-18 and ResNet-34 as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    class Variant(Enum):\n",
    "        RESNET18 = '18'\n",
    "        RESNET34 = '34'\n",
    "\n",
    "    _architectures = {\n",
    "        Variant.RESNET18: (BasicBlock, [2, 2, 2, 2]),\n",
    "        Variant.RESNET34: (BasicBlock, [3, 4, 6, 3])\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, variant='18'):\n",
    "        super(ResNet, self).__init__()\n",
    "        if not isinstance(variant, self.Variant):\n",
    "            variant = self.Variant(variant)\n",
    "        block, layers = self._architectures[variant]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55496634",
   "metadata": {},
   "source": [
    "### Experimental Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4080db",
   "metadata": {},
   "source": [
    "We used the following scripts to train and test the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf40348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "from dataset.dataloader import get_multilabel_dataloader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, max_grad_norm, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start = time()\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    duration = time() - start\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    logging.info(f\"epoch {epoch} - loss {avg_loss:.4f} - time {duration:.2f}s\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device, split):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    logging.info(f\"{split} loss {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device, label_dict_path):\n",
    "    \"\"\"\n",
    "    Calculate overall accuracy and recall for the disease class,\n",
    "    using the 'No Finding' label bit to distinguish normal vs. abnormal.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Load and parse label dictionary into a nameâ†’index map\n",
    "    with open(label_dict_path, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    label_to_idx = {}\n",
    "    for line in lines:\n",
    "        name, idx = line.split(\":\")\n",
    "        label_to_idx[name.strip()] = int(idx.strip())\n",
    "    # Get index of the 'No Finding' (normal) class\n",
    "    no_find_idx = label_to_idx[\"No Finding\"]\n",
    "    # Prepare counters\n",
    "    total_samples = 0\n",
    "    correct_preds = 0\n",
    "    true_positive = 0    # correctly predicted diseased\n",
    "    false_negative = 0   # diseased samples predicted as normal\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Model outputs logits for each class\n",
    "        outputs = model(images)\n",
    "        probs = sigmoid(outputs)\n",
    "        # Predicted normal if P(No Finding) >= 0.5\n",
    "        pred_normal = probs[:, no_find_idx] >= 0.5\n",
    "        # Ground-truth normal where target bit is 1\n",
    "        actual_normal = targets[:, no_find_idx] == 1\n",
    "        # Update accuracy count\n",
    "        correct_preds += (pred_normal == actual_normal).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "        # For disease (positive) samples (actual_normal == False)\n",
    "        disease_mask = ~actual_normal\n",
    "        # Predicted disease if not predicted normal\n",
    "        pred_disease = ~pred_normal\n",
    "        true_positive += (disease_mask & pred_disease).sum().item()\n",
    "        false_negative += (disease_mask & pred_normal).sum().item()\n",
    "    # Compute metrics\n",
    "    accuracy = correct_preds / total_samples if total_samples > 0 else 0.0\n",
    "    recall = (true_positive / (true_positive + false_negative)\n",
    "              if (true_positive + false_negative) > 0 else 0.0)\n",
    "    # Log results\n",
    "    logging.info(f\"Test Accuracy: {accuracy * 100:.3f}\")\n",
    "    logging.info(f\"Disease Recall: {recall * 100:.3f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train ViT on multi-label classification\")\n",
    "    parser.add_argument(\"--img_size\", type=int,\n",
    "                        default=224, help=\"Input image size\")\n",
    "    parser.add_argument(\"--ckpt_dir\", type=str,\n",
    "                        default=\"/home/users/nus/e0945822/scratch/checkpoints\", help=\"Checkpoint directory\")\n",
    "    parser.add_argument(\"--split_type\", type=str, default=\"balanced\", choices=[\n",
    "                        \"balanced\", \"rare_first\", \"original\", \"binary\"], help=\"Split type for dataset\")\n",
    "    parser.add_argument(\"--save_every\", type=int, default=10,\n",
    "                        help=\"Save checkpoint every N epochs\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50,\n",
    "                        help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int,\n",
    "                        default=32, help=\"Batch size\")\n",
    "    parser.add_argument(\"--device\", type=str,\n",
    "                        default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser.add_argument(\"--index\", type=int, default=0, help=\"GPU device ID\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42,\n",
    "                        help=\"Random seed for reproducibility\")\n",
    "    parser.add_argument(\"--test_only\", action=\"store_true\",\n",
    "                        help=\"Test only without training\")\n",
    "    parser.add_argument(\"--ckpt_path\", type=str, default=None,\n",
    "                        help=\"Absolute path to the checkpoint for testing\")\n",
    "    parser.add_argument(\"--model\", choices=[\"alexnet\", \"vgg16\", \"resnet18\", \"resnet34\", \"vit_pt\"],\n",
    "                        help=\"The model to run\")\n",
    "    args = parser.parse_args()\n",
    "    # Set logging configuration\n",
    "    log_dir = osp.join(\n",
    "        \"logs\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    )\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = osp.join(\n",
    "        log_dir,\n",
    "        f\"{args.model}\" + f\"{'-test' if args.test_only else ''}\" +\n",
    "        f\"-{args.split_type}-{datetime.now().strftime('%H-%M-%S')}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s: %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='w'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Get data loaders\n",
    "    logging.info(\"Loading data...\")\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.img_size == 224:\n",
    "        data_dir = \"/home/users/nus/e0945822/scratch/5242data/data_224/\"\n",
    "        if not os.path.exists(data_dir):\n",
    "            data_dir = \"data_tensor\"\n",
    "        train_loader = get_multilabel_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"train\",\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = get_multilabel_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"val\",\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        test_loader = get_multilabel_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"test\",\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid image size. Only supporting 224.\")\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    logging.info(\"Initializing model...\")\n",
    "    device = torch.device(args.device)\n",
    "    if args.device == \"cuda\":\n",
    "        device = torch.device(f\"cuda:{args.index}\")\n",
    "        torch.cuda.set_device(args.index)\n",
    "\n",
    "    model_args = {\n",
    "        \"in_channels\": 1,\n",
    "        \"num_classes\": 15\n",
    "    }\n",
    "\n",
    "    if args.model == 'alexnet':\n",
    "        from models.alexnet import AlexNet\n",
    "        model = AlexNet(**model_args).to(device)\n",
    "    elif args.model == 'vgg16':\n",
    "        from models.vgg import VGG\n",
    "        model = VGG(**model_args, variant='16').to(device)\n",
    "    elif args.model == 'resnet18' or 'resnet34':\n",
    "        from models.resnet import ResNet\n",
    "        variant = args.model.split('resnet')[0]\n",
    "        model = ResNet(**model_args, variant=variant).to(device)\n",
    "    elif args.model == 'vit_pt':\n",
    "        from models.vit_pt import ViT_PT\n",
    "        model = ViT_PT(**model_args).to(device)\n",
    "\n",
    "    if args.ckpt_path:\n",
    "        model.load_state_dict(torch.load(args.ckpt_path))\n",
    "        logging.info(f\"Loaded model from {args.ckpt_path}\")\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "    max_grad_norm = 1.0\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Main training loop\n",
    "    if not args.test_only:\n",
    "        logging.info(\"Starting training...\")\n",
    "        best_val_loss = float(\"inf\")\n",
    "        no_improve_epochs = 0\n",
    "        early_stop_patience = 10\n",
    "        ckpt_dir = osp.join(args.ckpt_dir, f\"{args.model}\" +\n",
    "                            f\"-{args.split_type}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\")\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train_one_epoch(model, train_loader, criterion,\n",
    "                            optimizer, max_grad_norm, device, epoch)\n",
    "            # Gradually improve the gradient clipping threshold\n",
    "            if epoch > 10:\n",
    "                max_grad_norm = min(max_grad_norm + 0.1, 5.0)\n",
    "\n",
    "            val_loss = validate(model, val_loader, criterion, device, \"val\")\n",
    "            scheduler.step(val_loss)\n",
    "            # Early stopping if no improvement or learning rate is too low\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_epochs = 0\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    osp.join(ckpt_dir, \"best_model.pt\")\n",
    "                )\n",
    "                logging.info(\n",
    "                    f\"Best model saved at epoch {epoch} with val loss {val_loss:.4f}\")\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if no_improve_epochs >= early_stop_patience or current_lr < 1e-6:\n",
    "                logging.info(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "            # Periodic checkpoint saving\n",
    "            if epoch % args.save_every == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    osp.join(ckpt_dir, f\"checkpoint_epoch{epoch}.pt\")\n",
    "                )\n",
    "                logging.info(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "        # Save the final model\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            osp.join(ckpt_dir, \"final_model.pt\")\n",
    "        )\n",
    "        logging.info(\"Final model saved.\")\n",
    "\n",
    "    # Testing\n",
    "    logging.info(\"Testing...\")\n",
    "    # Load the best model for testing\n",
    "    if not args.test_only:\n",
    "        ckpt_path = osp.join(ckpt_dir, \"best_model.pt\")\n",
    "    else:\n",
    "        if args.ckpt_path is None:\n",
    "            raise ValueError(\"Please provide a checkpoint path for testing.\")\n",
    "        ckpt_path = args.ckpt_path\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    logging.info(f\"Loaded tested model from {ckpt_path}\")\n",
    "    model.to(device)\n",
    "    test(model, test_loader, device, data_dir + \"/label_dict.txt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e00ea",
   "metadata": {},
   "source": [
    "The ViT model usually got its best validation accuracy at around 20 epochs, with the binary cross-entropy training and validation loss values both around $0.32$. After the first 20 epochs, the model starts to overfit, the training loss continues to decrease, while the validation loss starts to increase. The training process is usually stable, and the model converges well.\n",
    "\n",
    "We compared the performance of the ViT from scratch model under different sampling strategies, and the results are shown in the table below. The evaluation is based on the model checkpoint with the best performance on the validation set.\n",
    "\n",
    "| Sampling Strategy | Predict Acc (%) | Predict Recall (%) |\n",
    "| :---------------: | :-------------: | :----------------: |\n",
    "|     Original      |     62.91%      |       56.17%       |\n",
    "|    Round-robin    |     92.88%      |      100.00%       |\n",
    "|    Rare-first     |     100.00%     |      100.00%       |\n",
    "\n",
    "From the table, we can see that the round-robin sampling strategy significantly improved the model's performance, achieving an accuracy of 92.88% and a recall of 100%. The rare-first sampling strategy achieved perfect accuracy and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2bd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
