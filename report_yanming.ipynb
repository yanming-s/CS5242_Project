{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c125f493",
   "metadata": {},
   "source": [
    "## Sampling Strategies\n",
    "\n",
    "\n",
    "As we noticed that the data skewed heavily, we decided to use some sampling strategies to balance the dataset. The method we took included:\n",
    "\n",
    "1. **Original distribution sampling**: leveraging the multi-label stratified shuffle split functions provided by `iterstrat` library, we directly sampled the original distribution of the dataset. This sampling strategy is mainly used for verifying whether the skewed dataset will affect the model performance.\n",
    "\n",
    "2. **Round-robin sampling**: we sampled one instance from each label in a round-robin fashion until we reach the desired number of samples. When the number of samples of one label is expired, we will skip this label and continue sampling from the remaining labels. We used this method to try to sample the data samples as balanced as possible.\n",
    "\n",
    "3. **Rare-first sampling**: this method is used to sample the labels from the dataset in a rare-first manner. This means that we will sample the rarest label first. As the \"No Finding\" label samples take roughly 50% of the dataset, and belonged samples actually may not help the model to learn so much. Through this method, we hope the model can pay more attention to the rare labels.\n",
    "\n",
    "4. **Binary sampling**: as we want to leverage the generative models to do abnormality detection, we relabeled the dataset to binary labels. The samples with \"No Finding\" label are relabeled to 0, and the samples with other labels are relabeled to 1. This method is used to verify whether the generative models can learn the distribution of the normal samples.\n",
    "\n",
    "We trained the models with the above sampling strategies and evaluated the performance of the models on the test set, to compare the performance of different kinds of deep learning models, and how will the sampling strategies affect the model performance. The different sampling strategies also served as a data augmentation method to increase the diversity of the training set in our project.\n",
    "\n",
    "For each sampling strategy, the dataset is split into 80% for training (20k samples), 10% for validation (2.5k samples), and 10% for testing (2.5k samples). The training set is used to train the model, the validation set is used to avoid overfitting and trigger early stopping, and the test set is used to evaluate the model performance.\n",
    "\n",
    "Here we utilized the following visualization function to show the relative distribution of the labels in the preprocessed dataset under different sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def show_split_distribution(train_df, val_df, test_df, original_df=None, save_name=None):\n",
    "    \"\"\"\n",
    "    Visualizes the relative percentage distribution of labels across train, validation and test sets.\n",
    "    \"\"\"\n",
    "    # Function to count labels in a dataframe\n",
    "    def count_labels(df):\n",
    "        label_counts = defaultdict(int)\n",
    "        for labels in df['Finding Labels']:\n",
    "            for label in labels:\n",
    "                label_counts[label] += 1\n",
    "        return label_counts\n",
    "    # Count labels in each split\n",
    "    train_counts = count_labels(train_df)\n",
    "    val_counts = count_labels(val_df)\n",
    "    test_counts = count_labels(test_df)\n",
    "    # If original dataset is provided, count those labels too\n",
    "    if original_df is not None:\n",
    "        # Ensure 'Finding Labels' are lists\n",
    "        if isinstance(original_df[\"Finding Labels\"].iloc[0], str):\n",
    "            orig_df_copy = original_df.copy()\n",
    "            orig_df_copy[\"Finding Labels\"] = orig_df_copy[\"Finding Labels\"].str.split('|')\n",
    "        else:\n",
    "            orig_df_copy = original_df\n",
    "        orig_counts = count_labels(orig_df_copy)\n",
    "    # Get all unique labels\n",
    "    all_labels = set()\n",
    "    for counts in [train_counts, val_counts, test_counts]:\n",
    "        all_labels.update(counts.keys())\n",
    "    all_labels = sorted(list(all_labels))\n",
    "    # Prepare data for plotting\n",
    "    train_values = [train_counts.get(label, 0) for label in all_labels]\n",
    "    val_values = [val_counts.get(label, 0) for label in all_labels]\n",
    "    test_values = [test_counts.get(label, 0) for label in all_labels]\n",
    "    # Calculate percentages\n",
    "    train_size = sum(train_values)\n",
    "    val_size = sum(val_values)\n",
    "    test_size = sum(test_values)\n",
    "    train_pct = [count/train_size*100 for count in train_values]\n",
    "    val_pct = [count/val_size*100 for count in val_values]\n",
    "    test_pct = [count/test_size*100 for count in test_values]\n",
    "    # For original dataset if provided\n",
    "    if original_df is not None:\n",
    "        orig_values = [orig_counts.get(label, 0) for label in all_labels]\n",
    "        orig_size = sum(orig_values)\n",
    "        orig_pct = [count/orig_size*100 for count in orig_values]\n",
    "    # Setup for plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    x = np.arange(len(all_labels))\n",
    "    width = 0.2\n",
    "    plt.bar(x - width*1.5, train_pct, width, label='Train')\n",
    "    plt.bar(x - width/2, val_pct, width, label='Validation')\n",
    "    plt.bar(x + width/2, test_pct, width, label='Test')\n",
    "    if original_df is not None:\n",
    "        plt.bar(x + width*1.5, orig_pct, width, label='Original')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.title('Relative Label Distribution')\n",
    "    plt.xticks(x, all_labels, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_name:\n",
    "        save_dir = \"imgs\"\n",
    "        if not osp.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = osp.join(save_dir, save_name)\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "    # Print summary statistics\n",
    "    print(\"Dataset sizes:\")\n",
    "    print(f\"Train: {len(train_df)} samples with {sum(train_values)} labels (avg {sum(train_values)/len(train_df):.2f} labels per sample)\")\n",
    "    print(f\"Validation: {len(val_df)} samples with {sum(val_values)} labels (avg {sum(val_values)/len(val_df):.2f} labels per sample)\")\n",
    "    print(f\"Test: {len(test_df)} samples with {sum(test_values)} labels (avg {sum(test_values)/len(test_df):.2f} labels per sample)\")\n",
    "    if original_df is not None:\n",
    "        print(f\"Original: {len(original_df)} samples with {sum(orig_values)} labels (avg {sum(orig_values)/len(original_df):.2f} labels per sample)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5400a7",
   "metadata": {},
   "source": [
    "### Original Distribution Sampling\n",
    "\n",
    "To implement the original distribution sampling, we used the `MultiLabelStratifiedShuffleSplit` function from the `iterstrat` library. This function allows us to split the dataset into training, validation, and test sets while preserving the original label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457918c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_split(df, train_size=20000, val_size=2500, test_size=2500, seed=42):\n",
    "    random.seed(seed)\n",
    "    df = df[[\"Image Index\", \"Finding Labels\"]].copy()\n",
    "    if isinstance(df[\"Finding Labels\"].iloc[0], str):\n",
    "        df[\"Finding Labels\"] = df[\"Finding Labels\"].str.split(\"|\")\n",
    "    # Binarize the labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y_full = mlb.fit_transform(df[\"Finding Labels\"])   # (N, L)\n",
    "    idx_full = df.index.to_numpy()\n",
    "    total = len(df)\n",
    "    subsz = train_size + val_size + test_size\n",
    "    # Select a stratified subsample of the desired total size from the full dataset\n",
    "    msss0 = MultilabelStratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=subsz / total,\n",
    "        test_size=(total - subsz) / total,\n",
    "        random_state=seed\n",
    "    )\n",
    "    sub_i, _ = next(msss0.split(idx_full, Y_full))\n",
    "    df_sub = df.iloc[sub_i].reset_index(drop=True)\n",
    "    Y_sub  = Y_full[sub_i]\n",
    "    # Split the subsample into training and temporary (validation + test) sets\n",
    "    msss1 = MultilabelStratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=train_size / subsz,\n",
    "        test_size=(val_size + test_size) / subsz,\n",
    "        random_state=seed\n",
    "    )\n",
    "    tr_i, tmp_i = next(msss1.split(df_sub.index.to_numpy(), Y_sub))\n",
    "    df_train = df_sub.iloc[tr_i].reset_index(drop=True)\n",
    "    df_tmp   = df_sub.iloc[tmp_i].reset_index(drop=True)\n",
    "    Y_tmp    = Y_sub[tmp_i]\n",
    "    # Split the temporary set into validation and test sets\n",
    "    msss2 = MultilabelStratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=val_size / (val_size + test_size),\n",
    "        test_size=test_size / (val_size + test_size),\n",
    "        random_state=seed\n",
    "    )\n",
    "    v_i, te_i = next(msss2.split(df_tmp.index.to_numpy(), Y_tmp))\n",
    "    df_val  = df_tmp.iloc[v_i].reset_index(drop=True)\n",
    "    df_test = df_tmp.iloc[te_i].reset_index(drop=True)\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "df = pd.read_csv(\"data/Data_Entry_2017.csv\")\n",
    "train_df, val_df, test_df = multilabel_split(df)\n",
    "show_split_distribution(train_df, val_df, test_df, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa000367",
   "metadata": {},
   "source": [
    "From the figure above, we can see that the original distribution of the dataset is heavily skewed towards the \"No Finding\" label, which takes up about 50% of the dataset. The other labels are much less frequent, with some labels having only a few samples. Using the original distribution sampling strategy, all labels preserved their original distribution in the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50faf15f",
   "metadata": {},
   "source": [
    "### Round-Robin Sampling\n",
    "\n",
    "To make the dataset more balanced, we implemented a round-robin sampling strategy. This method samples one instance from each label in a round-robin fashion until we reach the desired number of samples. The round-robin sampling strategy helps to ensure that each label is represented in the training set, even if some labels are rare. We implemented this strategy using the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5585d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_balanced_split(df, train_size=20000, val_size=2500, test_size=2500, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a multilabel dataset into balanced training, validation, and test sets.\n",
    "\n",
    "    This function attempts to ensure that each label is represented as evenly as possible across the splits,\n",
    "    by iteratively sampling indices associated with each label until the desired split sizes are reached.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    df = df[[\"Image Index\", \"Finding Labels\"]].copy()\n",
    "    if isinstance(df[\"Finding Labels\"].iloc[0], str):\n",
    "        df[\"Finding Labels\"] = df[\"Finding Labels\"].str.split('|')\n",
    "    total_size = train_size + val_size + test_size\n",
    "    # Construct a mapping of labels to their indices\n",
    "    label2idxs = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        for label in row['Finding Labels']:\n",
    "            label2idxs[label].append(idx)\n",
    "    labels = list(label2idxs.keys())\n",
    "    # Records the index of the last sampled item for each label\n",
    "    label_ptr = {l: 0 for l in labels}\n",
    "    # Main sampling loop\n",
    "    sampled_idx = set()\n",
    "    last_sampled_count = 0\n",
    "    cut_pos = []\n",
    "    newly_sampled_idx = []\n",
    "    total_idxs = set(range(len(df)))\n",
    "    while len(sampled_idx) < total_size:\n",
    "        for label in labels:\n",
    "            ptr = label_ptr[label]\n",
    "            total_label_idx = label2idxs[label]\n",
    "            # Skip labels that have been fully traversed\n",
    "            if ptr >= len(total_label_idx):\n",
    "                continue\n",
    "            while ptr < len(total_label_idx):\n",
    "                xi = total_label_idx[ptr]\n",
    "                ptr += 1\n",
    "                if xi not in sampled_idx:\n",
    "                    sampled_idx.add(xi)\n",
    "                    newly_sampled_idx.append(xi)\n",
    "                    break\n",
    "            label_ptr[label] = ptr\n",
    "        # Check if have exhausted all labels\n",
    "        any_exhausted = False\n",
    "        for l in labels:\n",
    "            if label_ptr[l] >= len(label2idxs[l]):\n",
    "                any_exhausted = True\n",
    "                break\n",
    "        if any_exhausted or len(sampled_idx) >= total_size:\n",
    "            # Record the cut point\n",
    "            cut_pos.append((last_sampled_count, len(sampled_idx)))\n",
    "            last_sampled_count = len(sampled_idx)\n",
    "    # Fill in the remaining samples\n",
    "    remaining_needed = total_size - len(sampled_idx)\n",
    "    if remaining_needed > 0:\n",
    "        remain_pool = list(total_idxs - sampled_idx)\n",
    "        random.shuffle(remain_pool)\n",
    "        for xi in remain_pool[:remaining_needed]:\n",
    "            sampled_idx.add(xi)\n",
    "            newly_sampled_idx.append(xi)\n",
    "        if remaining_needed > 0:\n",
    "            cut_pos.append((last_sampled_count, len(sampled_idx)))\n",
    "            last_sampled_count = len(sampled_idx)\n",
    "    # Assign the sampled indices to the corresponding dataset\n",
    "    sampled_idx_list = list(sampled_idx)\n",
    "    random.shuffle(sampled_idx_list)\n",
    "    train_ids = sampled_idx_list[:train_size]\n",
    "    val_ids = sampled_idx_list[train_size: train_size + val_size]\n",
    "    test_ids = sampled_idx_list[train_size + val_size: train_size + val_size + test_size]\n",
    "    train_df = df.loc[train_ids].reset_index(drop=True)\n",
    "    val_df = df.loc[val_ids].reset_index(drop=True)\n",
    "    test_df = df.loc[test_ids].reset_index(drop=True)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "df = pd.read_csv(\"data/Data_Entry_2017.csv\")\n",
    "train_df, val_df, test_df = multilabel_balanced_split(df)\n",
    "show_split_distribution(train_df, val_df, test_df, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36edb7",
   "metadata": {},
   "source": [
    "Through this method, we can see that the distribution of the labels in the training set is much more balanced compared to the original distribution. The round-robin sampling strategy ensures that each label is represented in the training set, which can help the model learn better representations for all labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631eb499",
   "metadata": {},
   "source": [
    "### Rare-First Sampling\n",
    "\n",
    "As rare diseases are often overlooked, but important to detect, we implemented a rare-first sampling strategy. This method samples the rarest label first, ensuring that the model pays more attention to the rare labels. The rare-first sampling strategy is implemented using the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_rare_first_split(df, train_size=20000, val_size=2500, test_size=2500, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a multilabel dataset into train, validation, and test sets, prioritizing rare labels.\n",
    "\n",
    "    This function ensures that images containing rare labels are assigned to splits first, \n",
    "    helping to maximize the representation of infrequent labels in all splits.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    df = df[[\"Image Index\", \"Finding Labels\"]].copy()\n",
    "    if isinstance(df['Finding Labels'].iloc[0], str):\n",
    "        df['Finding Labels'] = df['Finding Labels'].str.split('|')\n",
    "    # Count label frequencies\n",
    "    label_counter = Counter(label for labels in df['Finding Labels'] for label in labels)\n",
    "    sorted_labels = sorted(label_counter.items(), key=lambda x: x[1])  # rarest first\n",
    "    # Build mapping\n",
    "    image_labels = dict(zip(df['Image Index'], df['Finding Labels']))\n",
    "    label_to_images = defaultdict(set)\n",
    "    for img, labels in image_labels.items():\n",
    "        for label in labels:\n",
    "            label_to_images[label].add(img)\n",
    "    # Sets to track assignment\n",
    "    train_images = set()\n",
    "    val_images = set()\n",
    "    test_images = set()\n",
    "    used_images = set()\n",
    "    def add_image(img, split):\n",
    "        if img not in used_images:\n",
    "            if split == \"train\":\n",
    "                train_images.add(img)\n",
    "            elif split == \"val\":\n",
    "                val_images.add(img)\n",
    "            elif split == \"test\":\n",
    "                test_images.add(img)\n",
    "            used_images.add(img)\n",
    "    total_needed = train_size + val_size + test_size\n",
    "    for label, _ in sorted_labels:\n",
    "        candidate_images = list(label_to_images[label] - used_images)\n",
    "        random.shuffle(candidate_images)\n",
    "        n_total = len(candidate_images)\n",
    "        # Skip if we've hit capacity\n",
    "        if len(train_images) >= train_size and len(val_images) >= val_size and len(test_images) >= test_size:\n",
    "            break\n",
    "        remaining_train = train_size - len(train_images)\n",
    "        remaining_val = val_size - len(val_images)\n",
    "        remaining_test = test_size - len(test_images)\n",
    "        available = min(n_total, remaining_train + remaining_val + remaining_test)\n",
    "        if available == 0:\n",
    "            continue\n",
    "        n_train = int(available * (train_size / total_needed))\n",
    "        n_val = int(available * (val_size / total_needed))\n",
    "        n_test = available - n_train - n_val\n",
    "        # Adjust to stay within global limits\n",
    "        n_train = min(n_train, remaining_train)\n",
    "        n_val = min(n_val, remaining_val)\n",
    "        n_test = min(n_test, remaining_test)\n",
    "        idx = 0\n",
    "        for img in candidate_images[idx:idx + n_train]:\n",
    "            add_image(img, \"train\")\n",
    "        idx += n_train\n",
    "        for img in candidate_images[idx:idx + n_val]:\n",
    "            add_image(img, \"val\")\n",
    "        idx += n_val\n",
    "        for img in candidate_images[idx:idx + n_test]:\n",
    "            add_image(img, \"test\")\n",
    "    # If not enough images, fill randomly from unused\n",
    "    all_images = set(df[\"Image Index\"])\n",
    "    unused_images = list(all_images - used_images)\n",
    "    random.shuffle(unused_images)\n",
    "    while len(train_images) < train_size and unused_images:\n",
    "        add_image(unused_images.pop(), \"train\")\n",
    "    while len(val_images) < val_size and unused_images:\n",
    "        add_image(unused_images.pop(), \"val\")\n",
    "    while len(test_images) < test_size and unused_images:\n",
    "        add_image(unused_images.pop(), \"test\")\n",
    "    # Build DataFrames\n",
    "    train_df = df[df[\"Image Index\"].isin(train_images)].reset_index(drop=True)\n",
    "    val_df = df[df[\"Image Index\"].isin(val_images)].reset_index(drop=True)\n",
    "    test_df = df[df[\"Image Index\"].isin(test_images)].reset_index(drop=True)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "df = pd.read_csv(\"data/Data_Entry_2017.csv\")\n",
    "train_df, val_df, test_df = multilabel_rare_first_split(df)\n",
    "show_split_distribution(train_df, val_df, test_df, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa9acb",
   "metadata": {},
   "source": [
    "As shown in the figure, the rare-first sampling strategy ensures that the rare labels are represented in the splits. However, the \"No Finding\" label is excluded, due to its high prevalence in the dataset. This sampling strategy can help the model learn better representations for the rare labels, which can be important for detecting rare diseases, while still allowing the dataset to be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d62c51",
   "metadata": {},
   "source": [
    "### Binary Sampling\n",
    "\n",
    "To leverage the generative models for abnormality detection, we relabeled the dataset to binary labels. The samples with \"No Finding\" label are relabeled to 0, and the samples with other labels are relabeled to 1. This method is used to verify whether the generative models can learn the distribution of the normal samples. As there are only two labels in the dataset, we did not provide the visualization for this sampling strategy.\n",
    "\n",
    "For the size of the dataset, we still used 20k samples for training, to ensure that the comparison is fair across different deep learning models. And we used 2.5k normal samples and 2.5k abnormal samples for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fa457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_split(df, train_size=20000, test_normal_size=2500, test_abnormal_size=2500, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame containing medical findings into training and test sets for binary classification.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    df = df[[\"Image Index\", \"Finding Labels\"]].copy()\n",
    "    if isinstance(df[\"Finding Labels\"].iloc[0], str):\n",
    "        df[\"Finding Labels\"] = df[\"Finding Labels\"].str.split('|')\n",
    "    # Ensure 'binary_label' column exists\n",
    "    if 'binary_label' not in df.columns:\n",
    "        df['binary_label'] = df['Finding Labels'].apply(lambda labels: 0 if labels == ['No Finding'] else 1)\n",
    "    # Separate into normal and abnormal\n",
    "    df_normal = df[df['binary_label'] == 0].copy()\n",
    "    df_abnormal = df[df['binary_label'] == 1].copy()\n",
    "    # Shuffle both datasets\n",
    "    df_normal = df_normal.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    df_abnormal = df_abnormal.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    # Check if we have enough data\n",
    "    if len(df_normal) < train_size:\n",
    "        raise ValueError(f\"Not enough normal samples: have {len(df_normal)}, need {train_size}\")\n",
    "    if len(df_abnormal) < (test_normal_size + test_abnormal_size):\n",
    "        raise ValueError(f\"Not enough abnormal samples: have {len(df_abnormal)}, need {test_normal_size, test_abnormal_size}\")\n",
    "    # Create the splits\n",
    "    train_df = df_normal.iloc[:train_size].reset_index(drop=True)\n",
    "    test_normal_df = df_normal.iloc[train_size:train_size+test_normal_size].reset_index(drop=True)\n",
    "    test_abnormal_df = df_abnormal.iloc[:test_abnormal_size].reset_index(drop=True)\n",
    "    return train_df, test_normal_df, test_abnormal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104209dd",
   "metadata": {},
   "source": [
    "## Preprocessing and Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a296d36",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing\n",
    "\n",
    "Due to the high resolution of the original images and computation resource limitations, we resized the images to $224 \\times 224$ pixels. This resizing helps in reducing the computational load and improving processing time for subsequent analysis. Additionally, to deal with the multi-label nature of the dataset, we used the multi-hot encoding technique to convert the labels into a binary format. We preprocessed the dataset using the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa719859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_image_paths(root_dir):\n",
    "    \"\"\"\n",
    "    Collect all image paths from the dataset.\n",
    "    \"\"\"\n",
    "    image_dict = {}\n",
    "    folders = glob(os.path.join(root_dir, 'images_*', 'images'))\n",
    "    for folder in folders:\n",
    "        all_images = glob(os.path.join(folder, '*.png'))\n",
    "        for path in all_images:\n",
    "            name = os.path.basename(path)\n",
    "            image_dict[name] = path\n",
    "    return image_dict\n",
    "\n",
    "def convert_img_to_tensor(split_df, image_dict, save_root, split_type):\n",
    "    \"\"\"\n",
    "    Convert images to tensor format and save them.\n",
    "    \"\"\"\n",
    "    output_size = 224\n",
    "    save_dir = osp.join(save_root, split_type)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    indices = split_df[\"Image Index\"].values.tolist()\n",
    "    if len(indices) == 0:\n",
    "        torch.save(torch.empty((0, 1, output_size, output_size)), osp.join(save_dir, \"images.pt\"))\n",
    "    tensor_list = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((output_size, output_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    for i in tqdm(range(len(indices)), desc=f\"Converting {split_type} images\"):\n",
    "        image = indices[i]\n",
    "        path = image_dict[image]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        tensor = transform(img)\n",
    "        tensor_list.append(tensor)\n",
    "    tensors = torch.stack(tensor_list)\n",
    "    torch.save(tensors, osp.join(save_dir, \"images.pt\"))\n",
    "\n",
    "def convert_label_to_tensor(split_df, label_dict, save_root, split_type):\n",
    "    \"\"\"\n",
    "    Convert labels to tensor format and save them.\n",
    "    \"\"\"\n",
    "    save_dir = osp.join(save_root, split_type)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    lbl_series = split_df[\"Finding Labels\"].tolist()\n",
    "    num_samples = len(lbl_series)\n",
    "    num_classes = len(label_dict)\n",
    "    if num_samples == 0:\n",
    "        # Deal with empty dataset\n",
    "        empty = torch.empty((0, num_classes), dtype=torch.float32)\n",
    "        torch.save(empty, osp.join(save_dir, \"labels.pt\"))\n",
    "        return\n",
    "    tensor_list = []\n",
    "    for lbls in tqdm(lbl_series, desc=f\"Converting {split_type} labels\"):\n",
    "        # Multi-hot encoding for multi-label classification\n",
    "        mh = torch.zeros(num_classes, dtype=torch.float32)\n",
    "        for l in lbls:\n",
    "            if l and l in label_dict:\n",
    "                mh[label_dict[l]] = 1.0\n",
    "        tensor_list.append(mh)\n",
    "    labels_tensor = torch.stack(tensor_list)\n",
    "    torch.save(labels_tensor, osp.join(save_dir, \"labels.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1644bd3",
   "metadata": {},
   "source": [
    "Taking the round-robin sampling strategy function above as an example, we leveraged the following code snippet to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2846835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train images: 100%|██████████| 20000/20000 [05:22<00:00, 62.01it/s]\n",
      "Converting train labels: 100%|██████████| 20000/20000 [00:00<00:00, 40119.51it/s]\n",
      "Converting val images: 100%|██████████| 2500/2500 [00:41<00:00, 60.45it/s]\n",
      "Converting val labels: 100%|██████████| 2500/2500 [00:00<00:00, 89489.56it/s]\n",
      "Converting test images: 100%|██████████| 2500/2500 [00:38<00:00, 65.16it/s]\n",
      "Converting test labels: 100%|██████████| 2500/2500 [00:00<00:00, 89054.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset, can change the split method as needed\n",
    "train_df, val_df, test_df = multilabel_balanced_split(df)\n",
    "# Generate unified label dict from all Finding Labels\n",
    "label_dict = None\n",
    "all_labels = set()\n",
    "for item in df[\"Finding Labels\"].tolist():\n",
    "    lbls = item.split(\"|\")\n",
    "    all_labels.update([l for l in lbls if l])\n",
    "all_labels = sorted(all_labels)\n",
    "label_dict = {label: idx for idx, label in enumerate(all_labels)}\n",
    "# Save label dict to a file\n",
    "label_dict_path = os.path.join(\"data_tensor\", \"label_dict.txt\")\n",
    "with open(label_dict_path, \"w\") as f:\n",
    "    for label, idx in label_dict.items():\n",
    "        f.write(f\"{label}: {idx}\\n\")\n",
    "# Collect image paths\n",
    "image_dict = collect_image_paths(\"data\")\n",
    "# Convert images into tensor format\n",
    "convert_img_to_tensor(train_df, image_dict, \"data_tensor\", \"train\")\n",
    "convert_img_to_tensor(val_df, image_dict, \"data_tensor\", \"val\")\n",
    "convert_img_to_tensor(test_df, image_dict, \"data_tensor\", \"test\")\n",
    "# Convert labels into tensor format\n",
    "convert_label_to_tensor(train_df, label_dict, \"data_tensor\", \"train\")\n",
    "convert_label_to_tensor(val_df, label_dict, \"data_tensor\", \"val\")\n",
    "convert_label_to_tensor(test_df, label_dict, \"data_tensor\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a1d31",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "In practical applications of deep learning models in the medical domain, it is essential to evaluate model performance using appropriate metrics. Due to the critical nature of medical responsibilities and the need for rigorous diagnostic standards, it is widely accepted that models should accurately detect abnormalities and defer the final diagnosis to human experts. This collaborative approach ensures reliable outcomes and upholds the highest standards of patient care.\n",
    "\n",
    "In this project, we adhere to this guiding principle. The model outputs a probability vector matching the shape of the multi-hot encoded labels, where each bit corresponds to a specific disease. If the probability for the **\"No Finding\"** label exceeds 0.5, the sample is classified as normal; otherwise, it is considered abnormal.\n",
    "\n",
    "Formally, a sample is labeled **negative** (i.e., normal) if the \"No Finding\" probability is greater than 0.5, and **positive** (i.e., abnormal) otherwise.\n",
    "\n",
    "We use the following metrics to evaluate model performance when evaluating and comparing the performance of different models:\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly predicted positive and negative samples out of all samples.\n",
    "\n",
    "2. **Recall**: The proportion of correctly predicted positive samples out of all actual positive samples.\n",
    "\n",
    "Both metrics are important for assessing model performance. And in medical applications, recall is particularly critical to minimize the risk of missing abnormal cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37767b8",
   "metadata": {},
   "source": [
    "### Training Managements\n",
    "\n",
    "Though the evaluation of the models is based on a binary classification concept, for the training process, we still used the multi-label classification loss function. The loss function is calculated using the binary cross-entropy loss function, which is suitable for multi-label classification tasks. The loss function is defined as follows:\n",
    "\n",
    "$$ \\mathcal{L} = -\\frac{1}{N \\cdot C} \\sum\\limits_{n=1}^{N} \\sum\\limits_{c=1}^{C} \\left[ y_{n,c} \\cdot \\log(p_{n,c}) + (1 - y_{n,c}) \\cdot \\log(1 - p_{n,c}) \\right] $$\n",
    "\n",
    "where $N$ is the number of samples, $C$ is the number of classes, $y_{n,c}$ is the true label of the sample $n$ for class $c$, and $p_{n,c}$ is the predicted probability of the sample $n$ for class $c$. In the detailed implementation, we used the `BCEWithLogitsLoss` function from the `torch.nn` module, which combines a sigmoid layer and the binary cross-entropy loss in one single class. This function is numerically more stable than using a plain Sigmoid followed by a BCELoss.\n",
    "\n",
    "Due to the large size of the dataset, and to make the training process more efficient and manageable, for all the models mentioned below, we used a unified Python script to train them. In the following sections, we may only provide the used code snippets and the test results with the trained models, but without the output within the Jupyter Notebook and log contents. The Python script is available at the [GitHub repository](https://github.com/yanming-s/CS5242_Project), and is totally reproducible.\n",
    "\n",
    "Following is an example full training script for the Vision Transformer (ViT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "from models.vit import ViT\n",
    "from models.vit_pre import ViT_Pre\n",
    "from dataset.dataloader import get_multilabel_dataloader\n",
    "from dataset.lazy_dataloader import get_lazy_dataloader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, max_grad_norm, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start = time()\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    duration = time() - start\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    logging.info(f\"epoch {epoch} - loss {avg_loss:.4f} - time {duration:.2f}s\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device, split):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    logging.info(f\"{split} loss {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device, label_dict_path):\n",
    "    \"\"\"\n",
    "    Calculate overall accuracy and recall for the disease class,\n",
    "    using the 'No Finding' label bit to distinguish normal vs. abnormal.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Load and parse label dictionary into a name→index map\n",
    "    with open(label_dict_path, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    label_to_idx = {}\n",
    "    for line in lines:\n",
    "        name, idx = line.split(\":\")\n",
    "        label_to_idx[name.strip()] = int(idx.strip())\n",
    "    # Get index of the 'No Finding' (normal) class\n",
    "    no_find_idx = label_to_idx[\"No Finding\"]\n",
    "    # Prepare counters\n",
    "    total_samples = 0\n",
    "    correct_preds = 0\n",
    "    true_positive = 0    # correctly predicted diseased\n",
    "    false_negative = 0   # diseased samples predicted as normal\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Model outputs logits for each class\n",
    "        outputs = model(images)\n",
    "        probs = sigmoid(outputs)\n",
    "        # Predicted normal if P(No Finding) >= 0.5\n",
    "        pred_normal = probs[:, no_find_idx] >= 0.5\n",
    "        # Ground-truth normal where target bit is 1\n",
    "        actual_normal = targets[:, no_find_idx] == 1\n",
    "        # Update accuracy count\n",
    "        correct_preds += (pred_normal == actual_normal).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "        # For disease (positive) samples (actual_normal == False)\n",
    "        disease_mask = ~actual_normal\n",
    "        # Predicted disease if not predicted normal\n",
    "        pred_disease = ~pred_normal\n",
    "        true_positive += (disease_mask & pred_disease).sum().item()\n",
    "        false_negative += (disease_mask & pred_normal).sum().item()\n",
    "    # Compute metrics\n",
    "    accuracy = correct_preds / total_samples if total_samples > 0 else 0.0\n",
    "    recall = (true_positive / (true_positive + false_negative)\n",
    "              if (true_positive + false_negative) > 0 else 0.0)\n",
    "    # Log results\n",
    "    logging.info(f\"Test Accuracy: {accuracy * 100:.3f}\")\n",
    "    logging.info(f\"Disease Recall: {recall * 100:.3f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description=\"Train ViT on multi-label classification\")\n",
    "    parser.add_argument(\"--img_size\", type=int, default=224, help=\"Input image size\")\n",
    "    parser.add_argument(\"--ckpt_dir\", type=str, default=\"checkpoints\", help=\"Checkpoint directory\")\n",
    "    parser.add_argument(\"--save_every\", type=int, default=10, help=\"Save checkpoint every N epochs\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser.add_argument(\"--index\", type=int, default=0, help=\"GPU device ID\")\n",
    "    parser.add_argument(\"--use_pretrained\", action=\"store_true\", help=\"Use pretrained model\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility\")\n",
    "    parser.add_argument(\"--test_only\", action=\"store_true\", help=\"Test only without training\")\n",
    "    parser.add_argument(\"--ckpt_path\", type=str, default=None, help=\"Absolute path to the checkpoint for testing\")\n",
    "    parser.add_argument(\n",
    "        \"--split_type\",\n",
    "        type=str,\n",
    "        default=\"balanced\",\n",
    "        choices=[\"balanced\", \"rare_first\", \"original\", \"binary\"],\n",
    "        help=\"Split type for dataset\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    # Set logging configuration\n",
    "    log_dir = osp.join(\n",
    "        \"logs\",\n",
    "        datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    )\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = osp.join(\n",
    "        log_dir,\n",
    "        f\"vit{'-pretrained' if args.use_pretrained else ''}\" + f\"{'-test' if args.test_only else ''}\" +\\\n",
    "            f\"-{args.split_type}-{datetime.now().strftime('%H-%M-%S')}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s: %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='w'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get data loaders\n",
    "    logging.info(\"Loading data...\")\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.img_size == 224:\n",
    "        data_dir = \"data_224\"\n",
    "        if not os.path.exists(data_dir):\n",
    "            data_dir = \"data_tensor\"\n",
    "        train_loader = get_multilabel_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"train\",\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = get_multilabel_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"val\",\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        test_loader = get_multilabel_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"test\",\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    elif args.img_size == 1024:\n",
    "        data_dir = \"data_1024\"\n",
    "        train_loader = get_lazy_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"train\",\n",
    "            chunk_size=32,\n",
    "            max_chunks_in_ram=25,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = get_lazy_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"val\",\n",
    "            chunk_size=32,\n",
    "            max_chunks_in_ram=25,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        test_loader = get_lazy_dataloader(\n",
    "            data_dir,\n",
    "            split_type=args.split_type,\n",
    "            split=\"test\",\n",
    "            chunk_size=32,\n",
    "            max_chunks_in_ram=25,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Invalid image size. Choose either 224 or 1024.\")\n",
    "        \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    logging.info(\"Initializing model...\")\n",
    "    device = torch.device(args.device)\n",
    "    if args.device == \"cuda\":\n",
    "        device = torch.device(f\"cuda:{args.index}\")\n",
    "        torch.cuda.set_device(args.index)\n",
    "    if args.use_pretrained:\n",
    "        model_args = {\n",
    "            \"img_size\": args.img_size,\n",
    "            \"in_channels\": 1,\n",
    "            \"num_classes\": 15,\n",
    "            \"embed_dim\": 768,\n",
    "            \"depth\": 12,\n",
    "            \"num_heads\": 12,\n",
    "            \"mlp_ratio\": 4.0,\n",
    "            \"dropout\": 0.0,\n",
    "            \"use_pretrained_blocks\": True\n",
    "        }\n",
    "        model = ViT_Pre(**model_args).to(device)\n",
    "    else:\n",
    "        model_args = {\n",
    "            \"img_size\": args.img_size,\n",
    "            \"patch_size\": 16,\n",
    "            \"in_channels\": 1,\n",
    "            \"num_classes\": 15,\n",
    "            \"embed_dim\": 768,\n",
    "            \"depth\": 12,\n",
    "            \"num_heads\": 12,\n",
    "            \"mlp_dim\": 768 * 4,\n",
    "            \"dropout\": 0.0\n",
    "        }\n",
    "        model = ViT(**model_args).to(device)\n",
    "    if args.ckpt_path:\n",
    "        model.load_state_dict(torch.load(args.ckpt_path))\n",
    "        logging.info(f\"Loaded model from {args.ckpt_path}\")\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "    max_grad_norm = 1.0\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Main training loop\n",
    "    if not args.test_only:\n",
    "        logging.info(\"Starting training...\")\n",
    "        best_val_loss = float(\"inf\")\n",
    "        no_improve_epochs = 0\n",
    "        early_stop_patience = 10\n",
    "        ckpt_dir = osp.join(args.ckpt_dir, f\"vit{'-pretrained' if args.use_pretrained else ''}\"+\\\n",
    "                            f\"-{args.split_type}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\")\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train_one_epoch(model, train_loader, criterion, optimizer, max_grad_norm, device, epoch)\n",
    "            # Gradually improve the gradient clipping threshold\n",
    "            if epoch > 10:\n",
    "                max_grad_norm = min(max_grad_norm + 0.1, 5.0)\n",
    "\n",
    "            val_loss = validate(model, val_loader, criterion, device, \"val\")\n",
    "            scheduler.step(val_loss)\n",
    "            # Early stopping if no improvement or learning rate is too low\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_epochs = 0\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    osp.join(ckpt_dir, \"best_model.pt\")\n",
    "                )\n",
    "                logging.info(f\"Best model saved at epoch {epoch} with val loss {val_loss:.4f}\")\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if no_improve_epochs >= early_stop_patience or current_lr < 1e-6:\n",
    "                logging.info(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "            # Periodic checkpoint saving\n",
    "            if epoch % args.save_every == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    osp.join(ckpt_dir, f\"checkpoint_epoch{epoch}.pt\")\n",
    "                )\n",
    "                logging.info(f\"Checkpoint saved at epoch {epoch}\")\n",
    "        \n",
    "        # Save the final model\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            osp.join(ckpt_dir, \"final_model.pt\")\n",
    "        )\n",
    "        logging.info(\"Final model saved.\")\n",
    "\n",
    "    # Testing\n",
    "    logging.info(\"Testing...\")\n",
    "    # Load the best model for testing\n",
    "    if not args.test_only:\n",
    "        ckpt_path = osp.join(ckpt_dir, \"best_model.pt\")\n",
    "    else:\n",
    "        if args.ckpt_path is None:\n",
    "            raise ValueError(\"Please provide a checkpoint path for testing.\")\n",
    "        ckpt_path = args.ckpt_path\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    logging.info(f\"Loaded tested model from {ckpt_path}\")\n",
    "    model.to(device)\n",
    "    test(model, test_loader, device, data_dir + \"/label_dict.txt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cf2de",
   "metadata": {},
   "source": [
    "## Transformer Models\n",
    "\n",
    "Vision Transformers (ViT) have gained significant attention in the field of computer vision due to their ability to capture long-range dependencies and contextual information. In this section, we implemented a ViT model from scratch and another using the pretrained weights from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30732024",
   "metadata": {},
   "source": [
    "### ViT from Scratch\n",
    "\n",
    "We implemented a Vision Transformer (ViT) model from scratch in PyTorch, following the original ViT-Base configuration. Specifically, the model consists of 12 transformer blocks with 12 attention heads each and a hidden size of 768.\n",
    "\n",
    "A 2D convolutional layer is used to generate patch embeddings with a patch size of 16 and an embedding dimension of 512. The MLP layers within each block use a hidden dimension 4 times the hidden size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Patch_Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Module to split image into patches and embed them\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        # x: [bs, in_channel, img_size, img_size]\n",
    "        x = self.proj(x)  # [bs, embed_dim, height/patch_size, width/patch_size]\n",
    "        x = x.flatten(2)  # [bs, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # [bs, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class Transformer_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder block\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) \n",
    "    def forward(self, x):\n",
    "        # x: [seq_length, batch_size, embed_dim]\n",
    "        # Self-attention block\n",
    "        x_norm = self.norm1(x)\n",
    "        x_att, _ = self.mha(x_norm, x_norm, x_norm)\n",
    "        x = x + self.dropout1(x_att)\n",
    "        # MLP block\n",
    "        x_norm = self.norm2(x)\n",
    "        x = x + self.mlp(x_norm)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_channels=1,\n",
    "            num_classes=15,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            num_heads=12,\n",
    "            mlp_dim=768*4,\n",
    "            dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = Patch_Embedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            Transformer_Layer(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "    def forward(self, x):\n",
    "        # x: [bs, in_channels, img_size, img_size]\n",
    "        bs = x.shape[0]\n",
    "        x = self.patch_embed(x)  # [bs, num_patches, embed_dim]\n",
    "        cls_tokens = self.cls_token.expand(bs, -1, -1)  # [bs, 1, embed_dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [bs, 1 + num_patches, embed_dim]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        # Transformer expects shape [seq_length, batch_size, embed_dim]\n",
    "        x = x.transpose(0, 1)\n",
    "        for block in self.transformer_layers:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[0]  # shape: [bs, embed_dim]\n",
    "        logits = self.head(cls_token_final)\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Instantiate the Vision Transformer\n",
    "model_args = {\n",
    "    \"img_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"in_channels\": 1,\n",
    "    \"num_classes\": 15,\n",
    "    \"embed_dim\": 768,\n",
    "    \"depth\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"mlp_dim\": 768 * 4,\n",
    "    \"dropout\": 0.0\n",
    "}\n",
    "model = ViT(**model_args)\n",
    "# Forward pass\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)  # [1, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e00ea",
   "metadata": {},
   "source": [
    "The ViT model usually got its best validation accuracy at around 20 epochs, with the binary cross-entropy training and validation loss values both around $0.32$. After the first 20 epochs, the model starts to overfit, the training loss continues to decrease, while the validation loss starts to increase. The training process is usually stable, and the model converges well.\n",
    "\n",
    "We compared the performance of the ViT from scratch model under different sampling strategies, and the results are shown in the table below. The evaluation is based on the model checkpoint with the best performance on the validation set.\n",
    "\n",
    "| Sampling Strategy | Predict Acc (%) | Predict Recall (%) |\n",
    "| :---------------: | :-------------: | :----------------: |\n",
    "|     Original      |     62.91%      |       56.17%       |\n",
    "|    Round-robin    |     92.88%      |      100.00%       |\n",
    "|    Rare-first     |     100.00%     |      100.00%       |\n",
    "\n",
    "From the table, we can see that the round-robin sampling strategy significantly improved the model's performance, achieving an accuracy of 92.88% and a recall of 100%. The rare-first sampling strategy achieved perfect accuracy and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88a390",
   "metadata": {},
   "source": [
    "## Generative Models\n",
    "\n",
    "Generative models have shown great promise in various applications, including image generation, text generation, and more. It also performs well in the abnormality detection task. In this section, we implemented generative models, including the Generative Adversarial Network (GAN) and the Denoising Diffusion Probabilistic Model (DDPM), to detect abnormalities in the medical images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c877aa",
   "metadata": {},
   "source": [
    "### Denoising Diffusion Probabilistic Model (DDPM)\n",
    "\n",
    "We implemented a Denoising Diffusion Probabilistic Model (DDPM) incorporating the U-Net architecture. The DDPM model is trained to learn the distribution of the normal samples, and it can be used to detect abnormalities by generating samples from the learned distribution.\n",
    "\n",
    "More specifically, after training on the normal samples, we hope the model can distinguish between normal and abnormal samples through the reconstruction loss of the backward diffusion process. As the model is trained to generate normal samples, it should have a much lower reconstruction loss for normal samples than for abnormal samples.\n",
    "\n",
    "The DDPM initial noise scheduler is set to 0.0001, and the final noise scheduler is set to 0.02, with 1000 diffusion steps. The model is trained for 1000 epochs on the 20k normal samples. The implementation of the DDPM model is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568186c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double convolution block used in UNet\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    Downscaling with maxpool then double conv\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling then double conv\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Time embedding for the UNet model\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture for noise prediction in DDPM\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            base_channels=64,\n",
    "            time_emb_dim=256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        # Initial convolution\n",
    "        self.inc = DoubleConv(in_channels, base_channels)\n",
    "        # Downsampling path\n",
    "        self.down1 = Down(base_channels, base_channels*2)\n",
    "        self.down2 = Down(base_channels*2, base_channels*4)\n",
    "        self.down3 = Down(base_channels*4, base_channels*8)\n",
    "        self.down4 = Down(base_channels*8, base_channels*8)\n",
    "        # Time embeddings for different levels\n",
    "        self.time_mlp1 = nn.Linear(time_emb_dim, base_channels*2)\n",
    "        self.time_mlp2 = nn.Linear(time_emb_dim, base_channels*4)\n",
    "        self.time_mlp3 = nn.Linear(time_emb_dim, base_channels*8)\n",
    "        self.time_mlp4 = nn.Linear(time_emb_dim, base_channels*8)\n",
    "        # Upsampling path with skip connections\n",
    "        self.up1 = Up(base_channels*16, base_channels*4)\n",
    "        self.up2 = Up(base_channels*8, base_channels*2)\n",
    "        self.up3 = Up(base_channels*4, base_channels)\n",
    "        self.up4 = Up(base_channels*2, base_channels)\n",
    "        # Final convolution\n",
    "        self.outc = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embedding(t)\n",
    "        # Downsampling\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x2 = x2 + self.time_mlp1(t_emb)[:, :, None, None]\n",
    "        x3 = self.down2(x2)\n",
    "        x3 = x3 + self.time_mlp2(t_emb)[:, :, None, None]\n",
    "        x4 = self.down3(x3)\n",
    "        x4 = x4 + self.time_mlp3(t_emb)[:, :, None, None]\n",
    "        x5 = self.down4(x4)\n",
    "        x5 = x5 + self.time_mlp4(t_emb)[:, :, None, None]\n",
    "        # Upsampling with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        # Final convolution\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    \"\"\"\n",
    "    Denoising Diffusion Probabilistic Model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        num_timesteps=1000,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.num_timesteps = num_timesteps\n",
    "        # Define beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        # Pre-calculate different terms for closed form\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        # Calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "        \n",
    "    def forward_diffusion(self, x_0, t):\n",
    "        \"\"\"\n",
    "        Forward diffusion process: q(x_t | x_0)\n",
    "        Takes an image and a timestep as input and returns a noisy version of the image\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        mean = self.sqrt_alphas_cumprod[t][:, None, None, None] * x_0\n",
    "        variance = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "        return mean + variance * noise, noise\n",
    "    \n",
    "    def sample_timesteps(self, n):\n",
    "        \"\"\"\n",
    "        Sample timesteps uniformly for training\n",
    "        \"\"\"\n",
    "        return torch.randint(low=1, high=self.num_timesteps, size=(n,), device=self.device)\n",
    "\n",
    "    def p_losses(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Training loss calculation\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        # Add noise to the input image according to the timestep\n",
    "        x_noisy, target = self.forward_diffusion(x_0, t)\n",
    "        # Predict the noise with the model\n",
    "        predicted_noise = self.model(x_noisy, t)\n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(predicted_noise, target)\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, x, optimizer):\n",
    "        \"\"\"\n",
    "        Perform a single training step\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Sample random timesteps\n",
    "        batch_size = x.shape[0]\n",
    "        t = self.sample_timesteps(batch_size)\n",
    "        # Calculate loss\n",
    "        loss = self.p_losses(x, t)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "# Example usage\n",
    "unet = UNet(in_channels=1, out_channels=1)\n",
    "ddpm = DDPM(unet)\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=1e-4)\n",
    "# Dummy input\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x_0 = torch.randn(32, 1, 224, 224).to(device)\n",
    "loss = ddpm.train_step(x_0, optimizer)\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020985c",
   "metadata": {},
   "source": [
    "At the end of the training process, the model achieved a training MSE reconstruction loss of around 0.0067. And we used the test set of both normal and abnormal samples to evaluate the model performance for five times. The mean MSE loss and the standard deviation of the five runs are shown in the table below.\n",
    "\n",
    "| Data Type | Mean MSE Loss for 5 Runs | Std for 5 Runs |\n",
    "| :-------: | :----------------------: | :------------: |\n",
    "|  Normal   |         0.00708          |    0.00028     |\n",
    "| Abnormal  |         0.00706          |    0.00044     |\n",
    "\n",
    "From the results, we can see that the DDPM model got a very similar MSE $p$ loss for both normal and abnormal samples, with the standard deviation of the abnormal samples being slightly larger. This actually indicates that the model is not able to distinguish between normal and abnormal samples, and the model is not able to learn the distribution of the normal samples.\n",
    "\n",
    "We think the reason for this is that, in the real-world medical diagnosis, the crucial part to detect the disease is only a small section of the image. However, for the DDPM model, it is trained to learn the distribution of the whole chest X-ray image. In this case, the slight difference in normal and abnormal samples is not enough for the model to learn the distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2bd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
